#' @title Analyze sonde drift/mis-cal data in the context of drift
#' @author Guy Litt
#' @description Reads in data generated by the nightly alerts for drift detection.
#' This script identifies
#' the longest temporal gap in the data and assumes that before the gap 
#' represents pre-calibration data (grp1) and after the gap represents post-cal (grp2). 
#' Characteristics (e.g. mean/std dev of pre-post S1-S2 differenced groups) are then
#' recalculated.
#' 
#' 
#' 
#' @details Matched sonde data generated by flow.rtrn.alrt.snde.dtct.drft.R, 
#' nightly in the IS-monitoring pipeline


#' @notes Should only consider data after Aug 1, 2019. Before that, the sonde
#' was likely not wiping at all sites, with site-by-site fixes occurring May through
#' July 2019.
#' 
# Changelog / contributions
#  2022-05-06 beginning to craft analysis


# TODO consider completely re-downloading all timeseries data. Some unexpected gaps exist (e.g. idx=2102; ARIK 2019-10-23 18:10-19:10 at 101.100 should be continuous, with 0 gaps, whereas 102.100 has a gap.)
#       This shouldn't affect results, but it may help with filtering what to keep, and classifying whether just one sonde was calibrated or both

# TODO change this logic to entirely use the maximum gap as the presumed calibration period
#      (rather than keeping pre-post readings that worked from cluster analysis)
# TODO and then ensure consistent calculations of pre-post metrics for all calibrations


# TODO consider further adapting flow.snde.dtct.drft.R to extract more information useful for drift WG

# TODO Should consider fulcrum data and field-recorded biofouling. Ignore drift analyses where those data indicate non-'light' biofouling.

# TODO test time gap calc, e.g. POSE_2021-03-31 should range from 13:02 to 16:15 ~=3hrsl (idx 600 671 709 767 830; rsltDiff ix 585 656 694 751 813)

# TODO consider filtering data with excessive time gaps

library(iRobert.base)  # Present on dev-2 as of Spring, 2022
library(eddycopipe) # Present on dev-2 as of Spring, 2022
library(alerts) # Present on dev-2 as of Spring, 2022


funcDir <- "~/R/NEON-drift-correction/pack/"
sapply(list.files(funcDir, pattern = ".R"), function(x) source(paste0(funcDir,x)))
# Read bucket
bucket <- iRobert.base::wrap.set.creds.based.on.server() # poorly designed function for general application. Must be on dev-2 or prod-2
metaDirXtra <- "metadata/sondeDrift/xtra/"

# Fulcrum data inside neon-dev-is-sensor-health-data bucket (only includs some fulcrum data):
datFlcm <- eddycopipe::wrap_neon_gcs_read(object = paste0("metadata/sondeDrift/ais_sonde_fulcrum_prod.rds"), bucket = bucket)

# (Includes all fulcrum data)
flcmCredPath = "/opt/apps/is_monitoring_pipeline/no_creds_in_the_repo/.fulcrum_creds.RDS" # path in dev-2
# Set fulcrum Creds:
if(base::file.exists(flcmCredPath)){
  CRED_FLCM <- base::readRDS(flcmCredPath)
} else {
  msgNoFlcmDbCred <- paste0("PROBLEM: The fulcrum credentials file does not exist in ", flcmCredPath)
  rlog$error(msgNoFlcmDbCred)
  print(msgNoFlcmDbCred)
  stop(msgNoFlcmDbCred)
}

# Write bucket:
bucketWrte <- "neon-dev-is-drift"

# Directory for saving intermediate work
saveDirTemp <- "~/analysesQAQC/drift/sondeDrift/"
if(!dir.exists(saveDirTemp)){
  dir.create(saveDirTemp,recursive=TRUE)
}

timeCol <- "timeRndMin"
nameS1 <- "data.101.100"
nameS2 <- "data.102.100"


# List data from neon-dev-is-sensor-health
allObjBckt <-  eddycopipe::neon_gcs_list_objects(bucket = bucket,
                                                 prefix = metaDirXtra)

aisSitz <- c()

dpDataSubStr <- base::gsub(metaDirXtra,"",allObjBckt$Key) %>% base::gsub(pattern = ".rds", replacement = "")
dtSplt <- lapply(dpDataSubStr, function(x) transpose(as.data.table(strsplit(x,split = "_")[[1]] ) )) %>%  rbindlist()
names(dtSplt) <- paste0(c("dpId","type","date"))

dtSplt$site <- unlist(lapply(dtSplt$dpId, function(x) strsplit(x, split=".",fixed=TRUE)[[1]][[2]]))
dtSplt$termStr <- unlist(lapply(dtSplt$dpId, function(x) strsplit(x, split=".",fixed=TRUE)[[1]][[6]]))
dtSplt$termNum <- stringr::str_extract_all(dtSplt$termStr, "\\(?[0-9,.]+\\)?","")

dtObj <- cbind(allObjBckt,dtSplt)

typzAnls <- unique(dtSplt$type)

clstRsltObj <- dtObj[grep("kMeansClstrRsltData", dtObj$Key), ]

if(!file.exists(paste(saveDirTemp,"rsltDiffSndeGrpClstReCalc.csv"))){
  lsRsltDiff <- base::list()
  
  for(idx in 1:nrow(clstRsltObj)){
    clstRslt <- eddycopipe::neon_gcs_get_rds(bucket = bucket,object = clstRsltObj$Key[idx]) %>% data.table::as.data.table()
    clstRslt$timeRndMin <- base::as.POSIXct(clstRslt$timeRndMin, tz = "UTC")
    
    # TODO Re-assign kMeans cluster values...
    namzRedo <- c("grp", "kMeanDiff","sdDiffClst",
                  "medianClst1_101.100","medianClst2_101.100",
                  "medianClst1_102.100","medianClst2_102.100")
    
    clstRslt <- as.data.frame(clstRslt) %>% select(-dplyr::all_of(namzRedo))
    

    
    #Extract information of interest:
    
    dfRsltDiff <- data.table::data.table()#matrix(nrow=1, ncol=1))
    # The last difference reading of group 1, and first difference reading of group 2
    dfRsltDiff$date <- as.Date(clstRslt$timeRndMin[1] )
    dfRsltDiff$site <- clstRsltObj$site[idx]
    dfRsltDiff$term <- clstRsltObj$termNum[idx]
    
    # ------------ Quantify the maintenance's duration, gapDur  -------------- #
    longData <- reshape2::melt(data = clstRslt,id.vars = timeCol,
                               measure.vars = c(nameS1,nameS2))
    
    # See if there are large time differences in gaps b/w S1 and S2. If so, something might be wrong.
    tDiffBoth <- longData %>% group_by(variable) %>% mutate(timeDiff = c(NA,diff(timeRndMin)))
    rsltMax <- tDiffBoth %>% group_by(variable) %>% summarise(max(timeDiff, na.rm = TRUE))
    if(abs(abs(rsltMax[1,2]) - abs(rsltMax[2,2])) > 20){
      warning(paste0("The S1 and S2 time gaps differ by ", abs(rsltMax[1,2]) - abs(rsltMax[2,2])," mins at ", clstRsltObj$Key[idx]))
    } 
    
    
    # longData <- longData[order(longData[[timeCol]]),]
    reWide <- reshape2::dcast(longData,formula= timeRndMin~variable, value.var = "value")
    reWide <- reWide[order(reWide[,timeCol]),]
    tDiff <- diff(reWide[[timeCol]],lag=1)
    
    flagMissDataNextDay <- 0
    # ------------------------------------------------------------------------ #
    if(max(tDiff) < 120){ # The gap is too short for a maintenance interval. 
      #                     Consider redownloading data, and the next day's worth of data too
      # in other words, If gapDur is too short, re-download data and broaden the window.
      
      # Reconstruct dp ID:
      dpId <- paste0("NEON.", substr(clstRsltObj$dpId[idx], start=1, stop = 28),
                     ".HOR.VER.000")
      timeBgn <- as.POSIXlt(dfRsltDiff$date)             
      timeEnd <- as.POSIXlt(dfRsltDiff$date + 2)
      
      # The following code borrowed from flow.rtrn.alrt.snde.dtct.drft.R in NEONScience/L0-tracking
      rptHorSnde <- alerts::def.snde.s1.s2.hor(siteID = dfRsltDiff$site,timeDat = timeBgn)
      
      # Ensure appropriate HOR.VER for AIS sites
      s1Dps <- base::gsub("HOR.VER",rptHorSnde$s1Loc,dpId)
      s2Dps <- base::gsub("HOR.VER",rptHorSnde$s2Loc,dpId)
      
      # GRAB DATA  (this is only one day and two 1-min streams, so okay for a presto pull)
      grabData <- base::suppressMessages(iRobert.base::def.get.l0.data(site = site,streams = c(s1Dps,s2Dps),startDate = timeBgn, endDate = timeEnd))
      
      # Separate data by stream:
      dataS1 <- base::lapply(s1Dps, function(dp) grabData[base::grep(dp, grabData$meas_strm_name),])
      base::names(dataS1) <- s1Dps
      dataS2 <- base::lapply(s2Dps, function(dp) grabData[base::grep(dp, grabData$meas_strm_name),])
      base::names(dataS2) <-  s2Dps
      # Just keep the list of dpId datasets with more than 1 row of data & rename
      totDatS1 <- unlist(lapply(names(dataS1), function(i) nrow(dataS1[[i]])>1))
           totDatS2 <- unlist(lapply(names(dataS2), function(i) nrow(dataS2[[i]])>1))

      
      if(!base::any(totDatS1) || !base::any(totDatS2)){
        # Skip this round (AND FLAG!!) - no useful extra data to analyze
        print(paste0("Empty dataset on the following day for ",
                     c(s1Dps, s2Dps)[which(c(totDatS1, totDatS2)==FALSE)], " on ",
                     timeBgn) )
        flagMissDataNextDay <- 1
        next()
      }
      
      datS1 <- dataS1[[totDatS1]]
      datS2 <- dataS2[[totDatS2]]
      names(datS1) <- c("timeChar","dpId","data.101.100")
      names(datS2) <- c("timeChar","dpId","data.102.100")
      # Round timestamps to the minute to facilitate merging
      datS1$timeRndMin <- lubridate::round_date(as.POSIXct(datS1$timeChar,tz="UTC"),
                                                unit = "min")
      datS2$timeRndMin <- lubridate::round_date(as.POSIXct(datS2$timeChar,tz="UTC"),
                                                unit = "min")
      # Remove un-needed cols and rename dpId col
      datS1 <- datS1 %>% select(-dplyr::all_of(c("timeChar","dpId")))
      datS2 <- datS2 %>% select(-dplyr::all_of(c("timeChar","dpId")))
      # datS1 <- datS1 %>% rename(dpIdS1=dpId)
      # datS2 <- datS2 %>% rename(dpIdS2=dpId)
      # Merging by rounded time
      datBoth <- merge(datS1,datS2, by ="timeRndMin") # A wide format
      
      if(base::nrow(datBoth) == 0){
        print(paste0(dpId, " has no overlapping S1/S2 data on ", timeBgn))
        next()
      }
      
      datBoth$diff <- datBoth$data.102.100 - datBoth$data.101.100
      
      # Now just assume the accuracy metrics don't change from rsltDiff
      datBoth$accS1 <- stats::median(clstRslt$accS1,na.rm=TRUE)
      datBoth$accS2 <- stats::median(clstRslt$accS2,na.rm=TRUE)
      datBoth$cmboAcc <- stats::median(clstRslt$cmboAcc,na.rm=TRUE)
      datBoth$termId <- base::strsplit(dpId,split = ".",fixed=TRUE)[[1]][7]
      datBoth$visit <- clstRslt$visit[1]   
   
      # Now combine the original clstRslt with the next day's datBoth
      # clstRslt <- bind_rows(clstRslt,datBoth) # when d/l just next day (not current + next day)
      clstRslt <- datBoth
      # Add in new data and re-assess for gaps
      # reWide <- bind_rows(reWide,datBoth[,names(reWide)])
      reWide <- datBoth[,names(reWide)]
      tDiff <- diff(reWide[[timeCol]],lag=1)
      
    } 
    # ------------------------------------------------------------------------ #
    
    idxPre <- which.max(tDiff)
    idxPost <- idxPre + 1
    endTimePre <- reWide[idxPre,timeCol]
    bgnTimePost <- reWide[idxPost,timeCol]
    
    gapDur <- difftime(bgnTimePost,endTimePre,units="mins")
    # TODO double check this is correct. S1 and S2 can have varying time gaps!!!
   
   
  # } # end for loop
    
    
    
    dfRsltDiff$endTimePre <- endTimePre
    dfRsltDiff$bgnTimePost <- bgnTimePost
    dfRsltDiff$gapDur <- gapDur
    dfRsltDiff$flagMissDataNextDay <- flagMissDataNextDay
    # TODO Based on gap duration, consider pre-cal and post-cal natural fluctuations within the same time duration
    
  
    # Redefine the cluster groups to represent pre-cal and post-cal
    clstRslt$grp <- NA
    clstRslt$grp[1:idxPre] <- 1
    clstRslt$grp[idxPost:nrow(clstRslt)] <- 2
    clstRslt$flagMissDataNextDay <- flagMissDataNextDay
    # --------- 
    idxsGrp2 <- which(clstRslt$grp == 2)
    idxsGrp1 <- which(clstRslt$grp ==1)
    grp1 <- clstRslt[-idxsGrp2,]
    grp2 <- clstRslt[idxsGrp2,]
    maxGrp1 <- which.max(grp1$timeRndMin)
    minGrp2 <- which.min(grp2$timeRndMin)
    # --
  
    # TODO not sure if this is the correct calculation on mean of difference
    
    grp1$kMeanDiff <- base::mean(grp1$diff)
    grp2$kMeanDiff <- base::mean(grp2$diff)
    grp1$sdDiffClst <- stats::sd(grp1$diff)
    grp2$sdDiffClst <- stats::sd(grp2$diff)
    
    clstRslt$kMeanDiff <- NA
    clstRslt$sdDiffClst <- NA
    clstRslt[idxsGrp1,"kMeanDiff"] <- base::mean(grp1$diff)
    clstRslt[idxsGrp2,"kMeanDiff"] <- base::mean(grp2$diff)
    clstRslt[idxsGrp1,"sdDiffClst"] <- stats::sd(grp1$diff)
    clstRslt[idxsGrp2,"sdDiffClst"] <- stats::sd(grp2$diff)
    
    
    clstRslt$medianClst1_101.100 <- NA
    clstRslt$medianClst1_102.100 <- NA
    clstRslt$medianClst2_101.100 <- NA
    clstRslt$medianClst2_102.100 <- NA
    clstRslt[,"medianClst1_101.100"] <- stats::median(grp1$data.101.100)
    clstRslt[,"medianClst1_102.100"] <- stats::median(grp1$data.102.100)
    clstRslt[,"medianClst2_101.100"] <- stats::median(grp2$data.101.100)
    clstRslt[,"medianClst2_102.100"] <- stats::median(grp2$data.102.100)
  
    
    # Difference between pre-and-post cal readings
    dfRsltDiff$preDiff <- grp1$diff[maxGrp1]
    dfRsltDiff$postDiff <- grp2$diff[minGrp2]
    dfRsltDiff$changeSngl <-  dfRsltDiff$postDiff -  dfRsltDiff$preDiff
    
    dfRsltDiff$mednAccS1 <- stats::median(clstRslt$accS1)
    dfRsltDiff$mednAccS2 <- stats::median(clstRslt$accS2)
    dfRsltDiff$mednCmboAcc <- stats::median(clstRslt$cmboAcc)
    dfRsltDiff$flagMissDataNextDay <- flagMissDataNextDay
    
    if(grp2$timeRndMin[minGrp2] < grp1$timeRndMin[maxGrp1]){
      cat(paste0(dfRsltDiff$site," ",dfRsltDiff$term, " ",dfRsltDiff$date, " did not exhibit defined clusters."))
      dfRsltDiff$defnClstGrp <- FALSE  # cluster grouping poorly-defined.
      stop()
      # TODO attempt to define pre-post cal groupings differently (e.g. search for gaps)
      
    
      
      
      if(length(which(is.na(longData))) > 10){
        warning(paste0("More than 10 data points NA, index: ",idx) )
      }
      # Assumption: The longest time gap corresponds to sensor maintenance
      
    } else {
      dfRsltDiff$defnClstGrp <- TRUE # cluster grouping well-defined.
      
      if(all(maxGrp1-29 > 0)){
        dfRsltDiff$preDiffMedn30 <- median(grp1$diff[(maxGrp1-29):maxGrp1])
      } else {
        dfRsltDiff$preDiffMedn30 <- NA
      }
      
      if(all(minGrp2+29 <= nrow(minGrp2))){
        dfRsltDiff$postDiffMedn30 <- median(grp2$diff[minGrp2:(minGrp2+29)])
      } else {
        dfRsltDiff$postDiffMedn30 <- NA
      }
      
      dfRsltDiff$changeMedn30 <-  dfRsltDiff$postDiffMedn30 -  dfRsltDiff$preDiffMedn30
      dfRsltDiff$kMeanDiffGrp1 <- unique(grp1$kMeanDiff)
      dfRsltDiff$kMeanDiffGrp2 <- unique(grp2$kMeanDiff)
      dfRsltDiff$sdDiffClstGrp1 <- unique(grp1$sdDiffClst)
      dfRsltDiff$sdDiffClstGrp2 <- unique(grp2$sdDiffClst)
      
    }
  
    lsRsltDiff[[idx]] <- data.table::as.data.table(dfRsltDiff)
  }
  rsltDiff <- data.table::rbindlist(lsRsltDiff, fill=TRUE)
  
  write.csv(rsltDiff,paste0(saveDirTemp,"rsltDiffSndeGrpClstReCalc.csv"))
} else {
  # TODO Write results to drift bucket:
  # def.set.gcp.env(bucket=bucketWrite)
  rsltDiff <- read.csv(paste0(saveDirTemp,"rsltDiffSndeGrpClstReCalc.csv"))
}



# =========================================================================== #
#             Reduce dataset to times when calibration was performed
# =========================================================================== #

# The rsltDiffSndeGrpClstReCalc considers both cleaning-only bouts and re-cal bouts
# Use fulcrum data to distinguish between cleaning vs. recalibrations:
namzColsCalStus <- c("s1_post_cal_do","s2_post_cal_do",
"s1_post_cal_turbidity","s2_post_cal_turbidity",
"s1_post_cal_conductivity","s2_post_cal_conductivity",
"s1_post_cal_ph","s2_post_cal_ph",
"s1_post_cal_ph_2","s2_post_cal_ph_2",
"s2_post_cal_do_2","s2_post_cal_do_2",
"post_cal_conductivity_2","s2_post_cal_conductivity_2",
"s1_post_cal_turbidity_2","s2_post_cal_turbidity_2"
)
# namzColsCalStus <- c(namzColsCalStus, paste0(namzColsCalStus,"_2"))

flcmParaList = base::list(vec_col_namz = c("siteid", "date","location_stream",
                                           "level_of_s1_biofouling","level_of_s2_biofouling", "post_cleaning_do", "s2post_cleaning_do", "s1_post_cal_ph","s2_post_cal_ph","ph_comparison_successful",
                                           "sonde_calibrated",
                                           namzColsCalStus),
                          siteid = "ALL",
                          flcmDateCol = "date")

datFlcm <- alerts::def.grab.flcm.app(apiToken = CRED_FLCM, start_date= "2018-01-01",
                                     end_date = max(rsltDiff$date),
                                     vec_col_namz = flcmParaList$vec_col_namz,
                                     siteid = flcmParaList$siteid,
                                     dateCol = flcmParaList$flcmDateCol,
                                     verbose=FALSE,
                                     formId = "3be325c3-dc74-4010-8cf6-24ee0d447094")
datFlcm <- datFlcm[base::order(datFlcm[,flcmParaList$flcmDateCol]),]

# Any data point that had at least one calibration (S1, S2, or both) could be useful for drift analysis
# Some fulcrum records are incomplete in specifying whether sonde was calibrated
#  Fill in the gaps here
idxsNaCalb <- which(is.na(datFlcm$sonde_calibrated))
# Identify records that only happen if calibration was performed:
idxsCalb <- (unique(unlist(lapply(namzColsCalStus, function(x) which(!is.na(datFlcm[[x]]))))))
datFlcm[idxsNaCalb,"sonde_calibrated"] <- "N" # Set all NA to not-calibrated
datFlcm[idxsCalb,"sonde_calibrated"] <- "Y" # Evidence of calibration

# Total uncertain whether cal performed or not:
idxsNotSure <- which(is.na(datFlcm$sonde_calibrated))

idxsCal <-which(datFlcm$sonde_calibrated=="Y")#,idxsCalCondS1,idxsCalCondS2)))
# Site-dates calibration:
siteDatzCal <- unique(datFlcm[idxsCal, c("siteid","date")])
siteDatzCal$date <- as.Date(siteDatzCal$date)
siteDatzCal$siteDate <- paste0(siteDatzCal$siteid,"_",siteDatzCal$date)
rsltDiff$siteDate <- paste0(rsltDiff$site,"_",rsltDiff$date)


# idxsCal <- which(siteDatzCal$siteDate %in% rsltDiff$siteDate)

# View(cmboCalDiff)

# =========================================================================== #
#         Analyze sequence of calibration results by site               
# =========================================================================== #
# Subset to results that included calibration at S1 or S2 or both:

cmboCalDiff <- base::merge(rsltDiff,siteDatzCal, by ="siteDate")
idxsCal <- which(rsltDiff$siteDate %in% cmboCalDiff$siteDate)

rsltCal <- rsltDiff[idxsCal,]

if(any(units(rsltCal$gapDur)!="mins")){
  stop("Must convert all units to minutes")
}
# Visits with time gaps <40 mins may be faulty representations of calibrations/shouldn't be in here.
idxsTooShort <- which(rsltCal$gapDur < 40)
# rsltCal$gapDur[idxsTooShort]
rsltCalb <- rsltCal[-idxsTooShort,]

# Premise: for e/ site, look at sequence of pre-post calibration assessment results
# Classify whether something is drift or mis-calibration
# Then filter further by considering biofouling fulcrum records.




idxsAbovUncn <- which(rsltCalb$changeMedn30 > rsltCalb$mednCmboAcc)
idxsBeloUncn <- which(rsltCalb$changeMedn30 < rsltCalb$mednCmboAcc)

idxsWithInUncn <- which(abs(rsltCalb$changeMedn30) <= rsltCalb$mednCmboAcc)

rsltCalb$QFOutUncn <- NA
rsltCalb$QFOutUncn[idxsWithInUncn] <- 0
rsltCalb$QFOutUncn[c(idxsAbovUncn,idxsBeloUncn)] <- 1

idxsUnkn <- (which(is.na(rsltCalb$QFOutUncn)))







# GOAL: Identify situations of recurring drift within a site-sensor

grpCalb <- rsltCalb %>% group_by_at(dplyr::all_of(c(site,term))) %>% mutate(calbInt = c(NA,diff(date)) )


grpN <- c("site","term")
grpCalb <- rsltCalb %>% group_by(across(all_of(grpN))) %>% mutate(calbInt = c(NA,diff(date)) )

# hist(rsltCalb$calbInt,breaks = 30)

rsltCalb <- ungroup(grpCalb)

idxsConsec <- which(rsltCalb$calbInt<70)


# By looping across site-sensor, recurring drift patterns may be identified.
for(site in unique(rsltCalb$site)){
  
  rsltCalbSite <- rsltCalb[grep(site,rsltCalb$site),]
  
  for(term in unique(rsltCalbSite$term)){
    idxsTerm <- which(rsltCalbSite$term == term)
    subTrm <- rsltCalbSite[idxsTerm,]
    if(base::nrow(subTrm)==0){
      next()
    }
    
    
    
    
    # Now try to identify potential instances of drift, as differenced values
    #  beyond the accuracy reading. Note that YSI published accuracy is defined in the 
    # Para_EXO_accuracy.csv within the alerts package parameter files, and
    #  alerts::def.calc.acc.val() chooses the largest accuracy value b/w absolute
    # The combined accuracy between S1 and S2 is then calculated.
    
    hist(subTrm$changeMedn30)
    
    # Ensure data.frame ordered by time
    subTrm <- subTrm[order(subTrm$date),]
    
    # The indices of data falling outside of expected range
    idxsAbovUncn <- which(abs(subTrm$changeSngl) > subTrm$mednCmboAcc)
    idxsBeloUncn <- which(abs(subTrm$changeSngl) <= subTrm$mednCmboAcc)
    # Consider the direction of drift:
    idxsAbovUncnPos <- which(subTrm$changeSngl > subTrm$mednCmboAcc)
    idxsAbovUncnNeg <- which(subTrm$changeSngl < -subTrm$mednCmboAcc)
    
    
    diff(idxsAbovUncn)
    
    
    hist(subTrm$changeMedn30[idxsAbovUncn])  
    
  }
}


head(clstRsltObj$Key)

# TODO subset by site, calculate the direction of drift correction (t2 - t1 value), then 



unique(clstRslt$)








clstRsltMelt <- clstRslt %>% select(dplyr::all_of(c(timeCol,nameS1,nameS2))) %>%
  reshape2::melt(measure.vars = c(nameS1, nameS2))



ggplot2::ggplot(clstRsltMelt,aes_string(x=timeCol, y = "value", color = "variable")) + 
  geom_line()
